{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##                  INFORME DEL PROYECTO:  Implementación de un modelo de clasificación de tickets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17c5962dac9088a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Introducción\n",
    "\n",
    "\n",
    "El presente informe describe el desarrollo e implementación de un sistema basado en inteligencia artificial para la clasificación de tickets. Este proyecto tiene como objetivo automatizar la categorización de solicitudes, lo cual es un paso crucial para mejorar la eficiencia operativa en sectores como el soporte técnico y la gestión de servicios.\n",
    "\n",
    "El uso de modelos de aprendizaje automático en aplicaciones como esta no solo agiliza procesos manuales, sino que también reduce errores humanos y optimiza el tiempo de respuesta. Este informe presenta los antecedentes de la problemática, el desarrollo del proyecto y las conclusiones obtenidas.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5eb420954ed06981"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Antecedentes:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b538009f7495e555"
  },
  {
   "cell_type": "markdown",
   "source": [
    "La implementación de modelos de predicción y análisis ha sido un área clave en el campo de la ciencia de datos. Un ejemplo destacado es el proyecto de predicción de tiempo de taxis en Nueva York (NYC Taxi Dataset), que utiliza datos masivos para optimizar la logística y movilidad urbana. Al igual que en dicho proyecto, en la clasificación de tickets se requiere manejar grandes volúmenes de datos y extraer patrones útiles para mejorar la toma de decisiones.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80b88b1364122ed2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cosas en común de ambos proyecto:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9c4030b7c827c62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Procesamiento y análisis de datos no estructurados o semi-estructurados.\n",
    "Uso de herramientas de aprendizaje automático para la predicción y categorización.\n",
    "Aplicación de técnicas de preprocesamiento para limpiar, transformar y estructurar los datos para el modelo.\n",
    "En este caso, el enfoque está en los tickets de servicio, donde cada uno puede clasificarse en categorías predefinidas según su descripción, urgencia y otros parámetros relevantes.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a126231ad47f2bb5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Desarrollo\n",
    "\n",
    "1. Diseño y Planificación\n",
    "El desarrollo del proyecto inició con la identificación de los requisitos y la definición del objetivo: implementar un modelo de clasificación de tickets con una interfaz gráfica para su fácil uso. Se estructuraron dos componentes principales:\n",
    "\n",
    "API de predicción: Desarrollada con FastAPI, aloja el modelo de aprendizaje automático para clasificar tickets.\n",
    "Interfaz de usuario: Desarrollada con Streamlit, permite a los usuarios cargar tickets y recibir predicciones.\n",
    "\n",
    "\n",
    "2. Preparación del Dataset\n",
    "El dataset utilizado para entrenar el modelo contenía múltiples columnas, entre las cuales destacan:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa51208263469d63"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Descripción del problema.\n",
    "Categoría objetivo.\n",
    "Otros atributos como fecha de creación y prioridad.\n",
    "El dataset fue preprocesado aplicando técnicas como:\n",
    "\n",
    "Limpieza de texto para eliminar ruido (caracteres especiales, puntuación innecesaria).\n",
    "Tokenización y vectorización con TF-IDF para convertir texto en representaciones numéricas utilizables por el modelo.\n",
    "\n",
    "\n",
    "3. Entrenamiento del Modelo\n",
    "Se probaron varios algoritmos de clasificación, entre ellos:\n",
    "\n",
    "Regresión logística: Ofreció buenos resultados iniciales debido a su simplicidad y eficiencia.\n",
    "Random Forest: Seleccionado como modelo final debido a su capacidad para manejar datos complejos y capturar relaciones no lineales.\n",
    "\n",
    "\n",
    "4. Desarrollo de la API\n",
    "La API se implementó utilizando FastAPI, lo que permitió exponer el modelo como un servicio web. Esta API acepta solicitudes en formato JSON, ejecuta predicciones y devuelve la categoría correspondiente.\n",
    "\n",
    "\n",
    "5. Creación de la Interfaz\n",
    "La interfaz gráfica fue construida con Streamlit, ofreciendo un diseño intuitivo donde los usuarios pueden:\n",
    "\n",
    "Subir descripciones de tickets.\n",
    "Obtener predicciones de la categoría correspondiente.\n",
    "Visualizar métricas de evaluación del modelo.\n",
    "\n",
    "\n",
    "6. Integración con Docker\n",
    "El sistema fue contenedorizado utilizando Docker para garantizar su portabilidad y consistencia en cualquier entorno de despliegue. Se creó un archivo docker-compose.yml para orquestar los contenedores de la API y la interfaz gráfica.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9390c7debe61c02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estructura del proyecto:\n",
    "\n",
    "Cuenta sobre la rama principal dos archvios .ipynb, uno es el ExamenFinal que contiene las instrucciones para realizar este trabajo y depsués está este informe escrito que explica un poco más a detalle el proceso que se siguió para elaborar este trabajo\n",
    "\n",
    "\n",
    "\n",
    "Después tenemos una carpeta llamada data que incluye el archivo json con los datos sin limpiar y adentro de data también hay una carpeta llamada processed que contiene un archivo .csv con los datos ya limpios y listos para trabajar y hacer análisis más a detalle.\n",
    "\n",
    "\n",
    "\n",
    "También tenemos una carpeta llamada Experiments donde se encuentran principalmente los notebooks de los modelos que se probaron para hacer predicciones, modelos como logistic regression, random forest y svc, donde también se hizo el registry de los modelos champion y challenger correspondientemente.\n",
    "\n",
    "\n",
    "\n",
    "Después en Notebooks que es otra carpeta contiene lo que es el EDA de los datos json sin limpiar, lo cual llevó a mostrar algunas distribuciones, gráficas y relaciones interesantes, por otro lado en Data Wrangling se encuentra el análisis un poco más profundo y eliminando carácteres que no eran relevantes para nuestro trabajo de predicción y también ahí se propuso la búsqueda de algunos modelos a utilizar.\n",
    "\n",
    "\n",
    "\n",
    "También  está la carpeta de training pipeline  donde se encuentra un script .py que hace la orquestación de los modelos designados con prefect y para después dejarlo listo para hacer el deploy. \n",
    "\n",
    "\n",
    "\n",
    "Por último tenemos la carpeta app que contiene dentro model y su respectivo Dockerfile, requierements y main.py que manda a llamar al mejor modelo para hacer predicciones.\n",
    "Y otra carpeta llamada Ui que contiene igual su dockerfile, requirements.txt y su main.py que pide los parámetros de entrada para la interfaz. Al final en app está el docker compose para levantar la Api. Gracias\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2b419e00c22ab48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusiones\n",
    "\n",
    "\n",
    "El proyecto de clasificación de tickets demostró ser una solución eficiente y escalable para automatizar procesos repetitivos en organizaciones. Entre los principales logros se encuentran:\n",
    "\n",
    "La implementación exitosa de un modelo de aprendizaje automático con un rendimiento robusto.\n",
    "La creación de una API y una interfaz gráfica intuitiva que facilitan el uso del sistema.\n",
    "La integración de herramientas modernas como Docker para asegurar la portabilidad y reproducibilidad.\n",
    "El enfoque seguido tiene un gran potencial de ser replicado en otras áreas, como análisis de texto en redes sociales, clasificación de correos electrónicos o asignación de tareas en sistemas de gestión empresarial.\n",
    "\n",
    "En proyectos futuros, se puede recomendar explorar técnicas más avanzadas como el uso de modelos basados en transformers (ej. BERT) para mejorar la comprensión semántica de los textos. Además, se podría ampliar la funcionalidad incorporando análisis en tiempo real y sistemas de priorización basados en aprendizaje profundo."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f82c4a27313bc278"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Referencias\n",
    "\n",
    "\n",
    "NYC Taxi Dataset: Análisis de movilidad urbana. Disponible en: https://www.nyc.gov\n",
    "Documentación de FastAPI: https://fastapi.tiangolo.com\n",
    "Herramientas de preprocesamiento de texto: Scikit-learn y NLTK."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "321fa088098306f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
