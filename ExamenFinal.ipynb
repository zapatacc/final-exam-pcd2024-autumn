{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "      ___   _\n",
    "     |_ _| | |_    ___   ___    ___\n",
    "      | |  | __|  / _ \\ / __|  / _ \\\n",
    "      | |  | |_  |  __/ \\__ \\ | (_) |\n",
    "     |___|  \\__|  \\___| |___/  \\___/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "<h1><strong>Classification Challenge: Mejorando la Gestión de Quejas con Machine Learning</strong></h1>\n",
    "\n",
    "<hr>\n",
    "<p>Bienvenid@ al Classification Challenge</p>\n",
    "<h2>Descripción</h2>\n",
    "\n",
    "<p>En el ámbito corporativo, enfrentar y resolver desafíos diarios es esencial para mejorar la experiencia del cliente y optimizar las operaciones. Un desafío común es la adecuada gestión y clasificación de las quejas de los clientes. Para abordar esta problemática de manera efectiva, disponemos de un dataset inicial que será empleado para entrenar un modelo de machine learning. Este modelo tiene como objetivo predecir la categoría adecuada para cada nueva queja recibida, utilizando el conocimiento derivado de casos anteriores.\n",
    "\n",
    "Es importante mencionar que el dataset proporcionado, denominado `tickets_classification_eng.json` (Puedes encontrar este dataset en la carpeta `data/raw_data`), no está limpio y requerirá un proceso de preparación antes de ser utilizado para el entrenamiento del modelo. Este dataset final deberá está formado por las siguientes columnas:\n",
    "\n",
    "    complaint_what_happened - El contenido textual de la queja, que proporciona detalles sobre el incidente o problema experimentado por el cliente.\n",
    "    ticket_classification - Una combinación de las categorías de producto y subproducto involucradas, que clasifica la queja en un contexto más amplio.\n",
    "\n",
    "Para asegurarnos de que el dataset esté listo para su uso, es crucial seguir los procedimientos que se expondran adelante para seleccionar, limpiar y preparar adecuadamente los datos. Una vez que se haya completado este proceso, será necesario guardar el dataset limpio para asegurar que el modelo de machine learning pueda ser entrenado con datos precisos y confiables. \n",
    "\n",
    "La implementación de este proyecto no solo busca mejorar la eficiencia en la gestión de quejas, sino también permitir que la empresa comprenda mejor las tendencias de los problemas reportados por los clientes, facilitando una respuesta más rápida y adecuada en el futuro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Hints\n",
    "- Utilice la función `json_normalize` del paquete `pandas` [aqui](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html) para importar los datos.\n",
    "- Use este código para importar el json como diccionario en `Python`:\n",
    "````\n",
    "import json\n",
    "with open(file_path, \"r\") as file:  \n",
    "    datos = json.load(file)\n",
    "````\n",
    "- Para la transformación de datos y obtener el dataset final a trabajar ejecute los comandos de pandas necesarios para aplicar el siguiente procedimiento:\n",
    "\n",
    "    1. **Selección de Columnas**:\n",
    "       Empieza seleccionando solo las columnas que contienen la información de la queja, el producto y el subproducto. Las columnas son:\n",
    "       - `_source.complaint_what_happened`\n",
    "       - `_source.product`\n",
    "       - `_source.sub_product`\n",
    "\n",
    "    2. **Renombrar Columnas**:\n",
    "       Cambia el nombre de las columnas para que sean más claras y fáciles de manejar:\n",
    "       - `_source.complaint_what_happened` a `complaint_what_happened`\n",
    "       - `_source.product` a `category`\n",
    "       - `_source.sub_product` a `sub_product`\n",
    "\n",
    "    3. **Creación de Nueva Columna**:\n",
    "       Añade una nueva columna llamada `ticket_classification` que sea el resultado de concatenar los valores de las columnas `category` y `sub_product`, separados por un signo más. Por ejemplo, si `category` contiene \"Banco\" y `sub_product` contiene \"Cuenta Corriente\", entonces `ticket_classification` debería ser \"Banco + Cuenta Corriente\".\n",
    "    \n",
    "    4. **Eliminar Columnas Redundantes**:\n",
    "       Después de crear la columna `ticket_classification`, elimina las columnas `sub_product` y `category`, ya que su información ahora está encapsulada en la nueva columna.\n",
    "    \n",
    "    5. **Limpieza de Datos en Columnas Específicas**:\n",
    "       Asegúrate de que la columna `complaint_what_happened` no contenga campos vacíos. Reemplaza esos campos vacíos con un valor que indique que los datos están ausentes (como `NaN`).\n",
    "    \n",
    "    6. **Eliminación de Filas con Datos Faltantes**:\n",
    "       Elimina todas las filas que tengan datos faltantes en las columnas críticas, es decir, `complaint_what_happened` y `ticket_classification`.\n",
    "    \n",
    "    7. **Reiniciar Índice**:\n",
    "       Finalmente, reinicia el índice del dataframe para asegurarte de que los índices sean consecutivos, lo cual es útil después de eliminar filas para mantener la consistencia y facilidad de acceso por índice.\n",
    "    8. **Guardar el DataFrame en un Archivo CSV**:\n",
    "   Guarda el DataFrame transformado en un archivo CSV. Elige un nombre de archivo que refleje el contenido del DataFrame y decide la ubicación más adecuada para guardar el archivo. Asegúrate de establecer el parámetro para no guardar el índice si no es necesario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPORTE ESCRITO - DAFNE TAMAYO LEÓN 744752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    " \n",
    "Este proyecto tiene como objetivo demostrar las habilidades adquiridas durante la clase **Proyecto de Ciencia de Datos**, impartida por el profesor [Cristian Camilo Zapata Zuluaga](https://github.com/zapatacc/), en el quinto semestre de la carrera de Ingeniería en Ciencia de Datos en el Instituto Tecnológico y de Estudios Superiores de Occidente (ITESO). A través de este trabajo, se busca realizar un análisis profundo de los datos, comprender la estructura de los proyectos de ciencia de datos, implementar modelos de *machine learning* para obtener predicciones óptimas y, además, dominar la orquestación de flujos mediante *pipelines* con Prefect para automatizar los procesos.\n",
    " \n",
    "### Índice\n",
    " \n",
    "1. **Entendimiento del problema**\n",
    " \n",
    "2. **Análisis Exploratorio de los Datos (EDA)**\n",
    "3. **Flujos de Datos y Estructura del Proyecto**\n",
    "   - Organización de archivos y carpetas\n",
    "   - Documentación técnica\n",
    "4. **Entrenamiento de Modelos**\n",
    "   - Experimentación y *hyperparameter tuning*\n",
    "   - Comparación y selección de modelos (*Champion* y *Challenger*)\n",
    "   - Registro en el Model Registry\n",
    "5. **Pipelines de Automatización y Tracking**\n",
    "   - Orquestación con Prefect\n",
    "   - Ejecución local y configuración con MLflow\n",
    "6. **Desarrollo de la Aplicación**\n",
    "   - **Interfaz de Usuario (Frontend)**:\n",
    "     - Diseño con Streamlit\n",
    "   - **Modelo y API (Backend)**:\n",
    "     - Implementación con FastAPI\n",
    "7. **Conclusiones y Reflexiones Finales**\n",
    " \n",
    "---\n",
    "### 1. Entendimiento del Problema\n",
    " \n",
    "El primer paso consistió en comprender el problema de inicio a fin. Esto requirió una lectura detallada de la descripción del caso, que se basa en el análisis de reseñas en línea para clasificar y predecir a qué grupo específico de productos pertenece cada reseña. En esencia, nos enfrentamos a un problema de **clasificación multiclase** que requiere el uso de herramientas de **Procesamiento de Lenguaje Natural (NLP)** para construir modelos de *machine learning* que clasifiquen las reseñas a partir de su texto.\n",
    " \n",
    "Adicionalmente, este análisis inicial permitió visualizar las necesidades de los usuarios finales y, por ende, planificar el desarrollo del **frontend**, priorizando una interfaz amigable e intuitiva para facilitar la interacción de diversos usuarios con la plataforma.\n",
    " \n",
    "---\n",
    " \n",
    "### 2. Análisis Exploratorio de los Datos (EDA)\n",
    " \n",
    "Para explorar y preparar los datos, se creó un *notebook* dedicado, disponible en `notebooks/eda.ipynb`. Este contiene todo el desarrollo del análisis exploratorio de los datos. A continuación, se destacan los puntos principales:\n",
    " \n",
    "- **Limpieza de los Datos**:  \n",
    "  Los datos presentaban inconsistencias y requerían un tratamiento específico. Algunos aspectos clave de este proceso fueron:\n",
    "  - **Normalización**: Los datos estaban almacenados en formato JSON, por lo que se normalizaron para convertirlos en un formato tabular (*dataframe*).\n",
    "  - **Filtrado de columnas relevantes**: Se seleccionaron únicamente las columnas necesarias para el análisis:  \n",
    "    - `\"_source.complaint_what_happened\"`  \n",
    "    - `\"_source.product\"`  \n",
    "    - `\"_source.sub_product\"`  \n",
    "  - **Renombrado de columnas**: Las columnas se renombraron para mejorar su legibilidad y facilidad de uso en los siguientes pasos.\n",
    " \n",
    "- **Ingeniería de Características**:  \n",
    "  Se llevó a cabo una pequeña etapa de ingeniería de características para ajustar los datos a los requisitos del modelo. El resultado final de este proceso dejó los datos en un formato limpio y estructurado, adecuado para su uso en las siguientes etapas del proyecto.\n",
    " \n",
    "- **Tratado de Nulos**:  \n",
    "  Durante la limpieza de los datos, se identificó un caso particular con valores nulos. Aunque inicialmente parecía que no había valores faltantes evidentes (`NaN`), se descubrió que ciertos campos contenían un valor vacío (`\"\"`), lo cual representa un carácter nulo (`'\\0'`).\n",
    " \n",
    "  Para manejar este problema:\n",
    "  - Se convirtieron estos valores vacíos explícitos a `NaN` para facilitar su identificación.\n",
    "  - Posteriormente, se realizó un filtrado para eliminar estos registros nulos y garantizar la calidad de los datos.  \n",
    " \n",
    "Este enfoque sistemático aseguró que los datos estuvieran preparados para entrenar modelos precisos y confiables, además de proporcionar información valiosa para el desarrollo del resto del proyecto.\n",
    " \n",
    " \n",
    "- **Ultimas modificaciones**:\n",
    " \n",
    "  Se detectaron varias categorias que tenian muy pocos registros (Algunas contaban solo con 1 registro) ya que no seria factile evaluar al modelo con ellas, se seleccionaron solo aquellas con por lo menos 100 registros, se que suena exagerdo pero sin la cantidad de registros suficientes son valores impredecibles que solo meteran ruido al modelo y haran que este ajuste peor\n",
    " \n",
    "---\n",
    " \n",
    "### 3. Flujo de Datos y Estructura del Proyecto\n",
    " \n",
    "El proyecto fue diseñado para facilitar la recepción de retroalimentación y la integración de nuevos datos para entrenamiento, asegurando un flujo eficiente y organizado. Para cumplir con este propósito, se definieron dos estructuras clave: la organización de los datos y la estructura de la aplicación.\n",
    " \n",
    "#### Organización de los Datos\n",
    " \n",
    "El proyecto cuenta con una carpeta principal denominada `data`, que contiene tres subdirectorios:\n",
    " \n",
    "1. **`raw_data`**:  \n",
    "   Almacena toda la información nueva que recibe el modelo como entrada del usuario. Esta es la fuente inicial de datos, aún sin procesar.\n",
    " \n",
    "2. **`preprocessed_data`**:  \n",
    "   Contiene los datos que ya han pasado por los procesos de normalización y selección de características descritos en el EDA. Estos datos están estructurados y preparados para análisis o entrenamiento preliminar.\n",
    " \n",
    "3. **`clean_data`**:  \n",
    "   Aquí se almacenan los datos en su forma más refinada. Estos han sido procesados para eliminar *stopwords*, caracteres no deseados y registros irrelevantes. Además, se ha realizado un filtrado para garantizar una cantidad equilibrada de registros por cada categoría a predecir.\n",
    " \n",
    "Esta organización permite un manejo claro y eficiente de los datos en diferentes etapas del proyecto, asegurando trazabilidad y facilidad de actualización.\n",
    " \n",
    "#### Estructura de la Aplicación\n",
    " \n",
    "La aplicación se organizó en un directorio llamado `app`, dividido en dos subdirectorios:\n",
    " \n",
    "1. **`UI`**:  \n",
    "   Diseñado para la interfaz de usuario, desarrollada con **Streamlit**. Aquí se concentran los archivos necesarios para crear una experiencia amigable y accesible para los usuarios finales.\n",
    " \n",
    "2. **`model`**:  \n",
    "   Contiene el backend de la aplicación, desarrollado con **FastAPI**. Este subdirectorio maneja las operaciones relacionadas con el modelo, incluyendo las inferencias y la interacción con los datos procesados.\n",
    " \n",
    "#### Integración con Docker\n",
    " \n",
    "Ambas estructuras fueron pensadas para facilitar la implementación de la aplicación en contenedores mediante **Docker**, permitiendo una creación, despliegue y monitoreo oportunos. Esta organización asegura que los componentes puedan ser actualizados o modificados de manera modular, mejorando la eficiencia y mantenibilidad del proyecto.\n",
    " \n",
    "### 4. Entrenamiento de Modelos y Selección del Mejor Modelo (*Champion*)\n",
    "Para ver mejor el apartado de entrenamiento del modelo puedes ir al notebook `notebooks/training.ipynb` donde podras ver el codigo comentado y explicado bagamente igual aqui te dejo un resumen de los puntos clave que permitieron la ejecucion completa del entrenamiento en local y en dagshub (__Importante recalcar que este no esta automatizado el automatizado lo veremos más adelante en el pipeline__)\n",
    " \n",
    "El enfoque para el entrenamiento de modelos incluyó los siguientes pasos principales, utilizando **GridSearchCV** para optimizar los hiperparámetros y **MLflow** para realizar el seguimiento y registro de experimentos.\n",
    " \n",
    "#### 1. **Preprocesamiento de los Datos**\n",
    "Se utilizó el campo de texto `complaint_what_happened` como entrada (`X`) y la columna `ticket_classification` como etiquetas de salida (`y`). Estas etiquetas fueron codificadas como valores enteros mediante un `LabelEncoder`. Posteriormente:\n",
    "- Los datos se dividieron en conjuntos de entrenamiento (60%) y prueba (40%), asegurando una distribución balanceada por clase usando `stratify`.\n",
    "- Para evitar problemas con clases desconocidas en el conjunto de prueba, se filtraron las instancias que no estaban presentes en las clases del conjunto de entrenamiento.\n",
    "- Se utilizó `TfidfVectorizer` para transformar el texto en representaciones numéricas (*TF-IDF*), generando matrices esparcidas para el modelo.\n",
    " \n",
    "#### 2. **Definición de Modelos y Búsqueda de Hiperparámetros**\n",
    "Se definieron varios modelos con sus respectivos hiperparámetros para explorar sus configuraciones óptimas:\n",
    "- **Regresión Logística**:\n",
    "  - Parámetros optimizados: `C` (regularización) y `penalty` (tipo de penalización).\n",
    "- **Random Forest**:\n",
    "  - Parámetros optimizados: `n_estimators`, `max_depth`, `min_samples_split`, y `min_samples_leaf`.\n",
    " \n",
    "Cada modelo se entrenó usando **GridSearchCV**, evaluando combinaciones de hiperparámetros mediante validación cruzada (*cross-validation*) con tres pliegues y utilizando `accuracy` como métrica de evaluación.\n",
    " \n",
    "#### 3. **Tracking con MLflow**\n",
    "Durante el entrenamiento, cada modelo se ejecutó en un experimento con seguimiento de MLflow:\n",
    "- Se registraron los **hiperparámetros óptimos** encontrados por GridSearchCV.\n",
    "- Se calcularon las métricas de desempeño en el conjunto de prueba, incluyendo:\n",
    "  - `accuracy`\n",
    "  - `precision`\n",
    "  - `recall`\n",
    "  - `f1_score` (promedio ponderado).\n",
    "- Se almacenó el mejor modelo encontrado como un artefacto en MLflow.\n",
    " \n",
    "#### 4. **Selección del Mejor Modelo (*Champion*)**\n",
    "Después de realizar los experimentos:\n",
    "- Los *runs* del experimento se recuperaron usando el cliente de MLflow.\n",
    "- Se evaluó el desempeño de cada modelo en términos de la métrica `accuracy`.\n",
    "- El modelo con la mayor precisión se identificó como el mejor modelo (*Champion*).\n",
    " \n",
    "#### 5. **Registro del Modelo en MLflow Model Registry**\n",
    "El mejor modelo se registró en el **Model Registry** de MLflow:\n",
    "- Se asignó un alias `champion` al modelo.\n",
    "- El modelo se transicionó a la etapa **Production**, asegurando que versiones anteriores fueran archivadas.\n",
    " \n",
    "#### Resultado Final\n",
    "El modelo con mejor desempeño quedó registrado como *Champion* y pasó a producción, listo para ser utilizado en la API de la aplicación. Este flujo garantiza trazabilidad y reproducibilidad en el desarrollo del modelo.\n",
    " \n",
    "---\n",
    " \n",
    "### 5. **Pipelines de Automatización y Tracking**\n",
    " \n",
    "En este proyecto, se implementó un flujo automatizado utilizando **Prefect** para orquestar las tareas de procesamiento, entrenamiento y selección del mejor modelo. Además, se integró **MLflow** para realizar el seguimiento y registro de experimentos, asegurando trazabilidad y facilidad en la transición del modelo a producción. Puedes revisar esto con mayor detalle y explicacion en el codigo `pipeline/train.py`\n",
    " \n",
    "#### **Descripción del Pipeline**\n",
    " \n",
    "El pipeline se organiza en varias etapas interconectadas, cada una con un propósito específico:\n",
    " \n",
    " \n",
    "#### **1. Lectura y Normalización de Datos**\n",
    "- **`readData`**: Carga los datos en formato JSON desde el directorio `raw_data`.\n",
    "- **`normalize`**: Normaliza el JSON en un `DataFrame` estructurado, adecuado para el preprocesamiento.\n",
    " \n",
    " \n",
    "#### **2. Preprocesamiento de Datos**\n",
    "- **`preprocessData`**:\n",
    "  - Filtra y renombra columnas relevantes (`complaint_what_happened`, `category`, `sub_product`).\n",
    "  - Genera una nueva columna `ticket_classification` combinando `category` y `sub_product`.\n",
    "  - Elimina registros con valores nulos y guarda los datos procesados en `preprocessed_data`.\n",
    " \n",
    "- **`cleanData`**:\n",
    "  - Realiza una limpieza adicional, eliminando caracteres no deseados (e.g., \"X\").\n",
    "  - Filtra clases con menos de 100 registros para garantizar la calidad del entrenamiento.\n",
    "  - Genera el archivo final `cleaned.csv` en el directorio `clean_data`.\n",
    " \n",
    " \n",
    "#### **3. Entrenamiento de Modelos**\n",
    "- **`trainingModel`**:\n",
    "  - Utiliza los datos limpios para dividirlos en conjuntos de entrenamiento y prueba, asegurando balance entre clases.\n",
    "  - Transforma los datos textuales en representaciones numéricas mediante *TF-IDF Vectorizer*.\n",
    "  - Entrena modelos con **GridSearchCV** para optimizar los hiperparámetros, probando:\n",
    "    - Regresión Logística\n",
    "    - Random Forest\n",
    "  - Calcula métricas de desempeño:\n",
    "    - `accuracy`\n",
    "    - `precision`\n",
    "    - `recall`\n",
    "    - `f1_score`\n",
    "  - Registra los mejores modelos y métricas en **MLflow**.\n",
    " \n",
    " \n",
    "#### **4. Selección del Mejor Modelo**\n",
    "- **`bestmodel`**:\n",
    "  - Recupera todos los *runs* del experimento actual en MLflow.\n",
    "  - Identifica el modelo con la mayor precisión (`accuracy`) y lo designa como *Champion*.\n",
    "  - Registra el modelo en el **Model Registry** de MLflow, asignándole:\n",
    "    - Alias: `champion`\n",
    "    - Etapa: `Production`\n",
    "  - Actualiza automáticamente el modelo en producción, archivando versiones anteriores.\n",
    " \n",
    " \n",
    "#### **5. Flujo Principal**\n",
    "- **`mainFlow`**:\n",
    "  - Orquesta todo el proceso, desde la lectura de datos hasta la actualización del modelo *Champion*.\n",
    "  - Los pasos incluyen:\n",
    "    1. Lectura y normalización de datos.\n",
    "    2. Preprocesamiento y limpieza.\n",
    "    3. Entrenamiento y registro de modelos.\n",
    "    4. Selección y despliegue del mejor modelo.\n",
    " \n",
    "---\n",
    " \n",
    "### 6. Desarrollo de la Aplicación\n",
    " \n",
    "La aplicación está diseñada para brindar una experiencia integrada, desde la interacción del usuario con la interfaz de frontend hasta el procesamiento de datos y predicción en el backend. Se compone de dos módulos principales:\n",
    " \n",
    "#### **Interfaz de Usuario (Frontend)**\n",
    "El frontend está desarrollado con **Streamlit**, una herramienta ideal para crear aplicaciones web de datos de manera rápida y sencilla. Su propósito es permitir a los usuarios interactuar con la aplicación ingresando texto y recibiendo predicciones sobre la categoría de sus quejas o comentarios.\n",
    " \n",
    "**Características:**\n",
    "- **Formulario de Entrada:** Los usuarios pueden describir su queja o incidente a través de un cuadro de texto.\n",
    "- **Botón de Predicción:** Una vez ingresado el texto, el botón activa el proceso de predicción.\n",
    "- **Respuesta en Tiempo Real:** Muestra la categoría predicha directamente en la interfaz tras enviar la solicitud al backend.\n",
    " \n",
    "#### **Modelo y API (Backend)**\n",
    "El backend está construido con **FastAPI**, una herramienta moderna y eficiente para desarrollar APIs robustas y rápidas. Su función principal es recibir solicitudes desde el frontend, procesar los datos de entrada mediante el modelo entrenado, y devolver la predicción correspondiente.\n",
    " \n",
    "**Características:**\n",
    "- **Recepción de Datos:** Utiliza un modelo de datos definido con **Pydantic** para validar la entrada.\n",
    "- **Carga Dinámica del Modelo:** Integra con **MLflow** para cargar dinámicamente el modelo etiquetado como `champion` en el registro de modelos.\n",
    "- **Transformación de Datos:** Utiliza un codificador para transformar las predicciones numéricas del modelo en etiquetas legibles para el usuario.\n",
    "- **Endpoint `/predict`:** Permite la interacción entre el frontend y el modelo, devolviendo resultados en formato JSON.\n",
    " \n",
    "#### **Integración y Ejecución**\n",
    "Ambos módulos están integrados mediante un archivo `docker-compose.yml`, que orquesta los contenedores de frontend y backend para asegurar una interacción fluida. Los servicios están configurados de la siguiente manera:\n",
    "- **Frontend:** Disponible en el puerto `8501` para recibir entradas del usuario.\n",
    "- **Backend:** Disponible en el puerto `8000` para procesar las solicitudes del frontend.\n",
    " \n",
    "**Flujo de Interacción:**\n",
    "1. El usuario ingresa un texto en el frontend.\n",
    "2. El texto es enviado como una solicitud POST al endpoint `/predict` del backend.\n",
    "3. El backend utiliza el modelo para realizar la predicción y devuelve la categoría predicha.\n",
    "4. El frontend muestra el resultado al usuario.\n",
    " \n",
    "Esta arquitectura asegura un flujo de trabajo claro, modular y escalable, permitiendo futuras actualizaciones en el modelo o la interfaz sin afectar otros componentes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
