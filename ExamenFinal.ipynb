{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "      ___   _\n",
    "     |_ _| | |_    ___   ___    ___\n",
    "      | |  | __|  / _ \\ / __|  / _ \\\n",
    "      | |  | |_  |  __/ \\__ \\ | (_) |\n",
    "     |___|  \\__|  \\___| |___/  \\___/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "<h1><strong>Classification Challenge: Mejorando la Gestión de Quejas con Machine Learning</strong></h1>\n",
    "\n",
    "<hr>\n",
    "<p>Bienvenid@ al Classification Challenge</p>\n",
    "<h2>Descripción</h2>\n",
    "\n",
    "<p>En el ámbito corporativo, enfrentar y resolver desafíos diarios es esencial para mejorar la experiencia del cliente y optimizar las operaciones. Un desafío común es la adecuada gestión y clasificación de las quejas de los clientes. Para abordar esta problemática de manera efectiva, disponemos de un dataset inicial que será empleado para entrenar un modelo de machine learning. Este modelo tiene como objetivo predecir la categoría adecuada para cada nueva queja recibida, utilizando el conocimiento derivado de casos anteriores.\n",
    "\n",
    "Es importante mencionar que el dataset proporcionado, denominado `tickets_classification_eng.json` (Puedes encontrar este dataset en la carpeta `data/raw_data`), no está limpio y requerirá un proceso de preparación antes de ser utilizado para el entrenamiento del modelo. Este dataset final deberá está formado por las siguientes columnas:\n",
    "\n",
    "    complaint_what_happened - El contenido textual de la queja, que proporciona detalles sobre el incidente o problema experimentado por el cliente.\n",
    "    ticket_classification - Una combinación de las categorías de producto y subproducto involucradas, que clasifica la queja en un contexto más amplio.\n",
    "\n",
    "Para asegurarnos de que el dataset esté listo para su uso, es crucial seguir los procedimientos que se expondran adelante para seleccionar, limpiar y preparar adecuadamente los datos. Una vez que se haya completado este proceso, será necesario guardar el dataset limpio para asegurar que el modelo de machine learning pueda ser entrenado con datos precisos y confiables. \n",
    "\n",
    "La implementación de este proyecto no solo busca mejorar la eficiencia en la gestión de quejas, sino también permitir que la empresa comprenda mejor las tendencias de los problemas reportados por los clientes, facilitando una respuesta más rápida y adecuada en el futuro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Hints\n",
    "- Utilice la función `json_normalize` del paquete `pandas` [aqui](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html) para importar los datos.\n",
    "- Use este código para importar el json como diccionario en `Python`:\n",
    "````\n",
    "import json\n",
    "with open(file_path, \"r\") as file:  \n",
    "    datos = json.load(file)\n",
    "````\n",
    "- Para la transformación de datos y obtener el dataset final a trabajar ejecute los comandos de pandas necesarios para aplicar el siguiente procedimiento:\n",
    "\n",
    "    1. **Selección de Columnas**:\n",
    "       Empieza seleccionando solo las columnas que contienen la información de la queja, el producto y el subproducto. Las columnas son:\n",
    "       - `_source.complaint_what_happened`\n",
    "       - `_source.product`\n",
    "       - `_source.sub_product`\n",
    "\n",
    "    2. **Renombrar Columnas**:\n",
    "       Cambia el nombre de las columnas para que sean más claras y fáciles de manejar:\n",
    "       - `_source.complaint_what_happened` a `complaint_what_happened`\n",
    "       - `_source.product` a `category`\n",
    "       - `_source.sub_product` a `sub_product`\n",
    "\n",
    "    3. **Creación de Nueva Columna**:\n",
    "       Añade una nueva columna llamada `ticket_classification` que sea el resultado de concatenar los valores de las columnas `category` y `sub_product`, separados por un signo más. Por ejemplo, si `category` contiene \"Banco\" y `sub_product` contiene \"Cuenta Corriente\", entonces `ticket_classification` debería ser \"Banco + Cuenta Corriente\".\n",
    "    \n",
    "    4. **Eliminar Columnas Redundantes**:\n",
    "       Después de crear la columna `ticket_classification`, elimina las columnas `sub_product` y `category`, ya que su información ahora está encapsulada en la nueva columna.\n",
    "    \n",
    "    5. **Limpieza de Datos en Columnas Específicas**:\n",
    "       Asegúrate de que la columna `complaint_what_happened` no contenga campos vacíos. Reemplaza esos campos vacíos con un valor que indique que los datos están ausentes (como `NaN`).\n",
    "    \n",
    "    6. **Eliminación de Filas con Datos Faltantes**:\n",
    "       Elimina todas las filas que tengan datos faltantes en las columnas críticas, es decir, `complaint_what_happened` y `ticket_classification`.\n",
    "    \n",
    "    7. **Reiniciar Índice**:\n",
    "       Finalmente, reinicia el índice del dataframe para asegurarte de que los índices sean consecutivos, lo cual es útil después de eliminar filas para mantener la consistencia y facilidad de acceso por índice.\n",
    "    8. **Guardar el DataFrame en un Archivo CSV**:\n",
    "   Guarda el DataFrame transformado en un archivo CSV. Elige un nombre de archivo que refleje el contenido del DataFrame y decide la ubicación más adecuada para guardar el archivo. Asegúrate de establecer el parámetro para no guardar el índice si no es necesario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"mediumvioletred\">REPORTE ESCRITO - DAFNE TAMAYO LEÓN 744752</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pasos para la resolución del examen:\n",
    " \n",
    "<font color=\"thistle\">\n",
    "  <ol>\n",
    "    <li>Ingeniería de características - Data wrangling</li>\n",
    "    <li>Análisis exploratorio de los datos</li>\n",
    "    <li>Entrenamiento, validación y evaluación de modelos</li>\n",
    "    <li>Automatización y orquestación del flujo de entrenamiento</li>\n",
    "    <li>Desarrollo de la API</li>\n",
    "    <li>Desarrollo del frontend</li>\n",
    "    <li>Contenerización del proyecto</li>\n",
    "    <li>Conclusiones</li>\n",
    "  </ol>\n",
    "</font>\n",
    "\n",
    "‎ \n",
    "‎\n",
    " \n",
    "\n",
    "<font color=\" steelblue\"> 1. **Ingeniería de características - Data wrangling** </font>\n",
    "\n",
    "**Archivo: `notebooks/analisis.ipynb`** \n",
    "\n",
    "- Primero importé el JSON como un diccionario usando `json.load` y lo normalicé con `json_normalize` para trabajar con los datos. Seleccioné solo las columnas necesarias: `_source.complaint_what_happened`, `_source.product` y `_source.sub_product`. \n",
    "- Luego, las renombré a `complaint_what_happened`, `category` y `sub_product` para que fueran más claras. \n",
    "- Creé una nueva columna llamada `ticket_classification`, donde concatené los valores de `category` y `sub_product` separados por un \"+\". \n",
    "- Eliminé las columnas `category` y `sub_product` porque ya no eran necesarias. \n",
    "- Sustituí los nulos en `complaint_what_happened` como NaN. \n",
    "- Después, eliminé las filas con datos faltantes en `complaint_what_happened` y `ticket_classification`, reinicié los índices para mantener el orden.\n",
    "- Al final nos quedamos con las categorías que tenían 200 o más registros y nos quedamos con 15 de las 78 categorías.\n",
    "- Y guardé el DataFrame transformado en un archivo CSV, sin guardar los índices.\n",
    "\n",
    "‎\n",
    "\n",
    "\n",
    "\n",
    "<font color=\" slateblue\"> 2. **Análisis exploratorio de los datos**  </font>\n",
    "\n",
    "**Archivo: `notebooks/analisis.ipynb`**\n",
    "\n",
    "- Realicé un análisis del dataset para identificar patrones, outliers y relaciones entre las variables.  \n",
    "- Generé visualizaciones que ayudaron a comprender la distribución y correlación de las variables relevantes.\n",
    "\n",
    "‎\n",
    "\n",
    "\n",
    "<font color=\"pink\">3. **Entrenamiento, validación y evaluación de modelos**  </font>\n",
    "\n",
    "**Archivo: `experiments/experiments.ipynb`**\n",
    "\n",
    "  - Dividí el dataset en un 80% para entrenamiento y un 20% para prueba, para que las clases estuvieran equilibradas   \n",
    "   - Usé MLflow para realizar el seguimiento de los experimentos con varios algoritmos, registrando métricas como `accuracy`, `precision` y `f1_score`  \n",
    "   - Ajusté los hiperparámetros de los modelos mediante GridSearch, que me permitió optimizar los modelos ajustando los mejores parámetros para el modelo\n",
    "   - Seleccioné el mejor modelo como *Champion* y el segundo mejor como *Challenger*, registrándolos en el Model Registry de MLflow\n",
    "\n",
    "‎\n",
    "\n",
    "\n",
    "<font color=\"palevioletred\">4. **Automatización y orquestación del flujo de entrenamiento** </font>\n",
    "\n",
    "**Archivo: `training_pipeline/training.py`**\n",
    "\n",
    "  - Diseñé e implementé un flujo de trabajo automatizado para la clasificación de quejas utilizando Prefect como herramienta de orquestación y MLflow para el seguimiento y registro de experimentos.\n",
    "   - Configuré MLflow para realizar tracking y registro automático de los modelos entrenados.    \n",
    "\n",
    "El entrenamiento de modelos se realiza mediante la tarea trainModels, donde se implementan tres algoritmos de clasificación: Regresión Logística, Máquinas de Soporte Vectorial (SVM) y K-Nearest Neighbors (KNN). Cada modelo es entrenado y evaluado dentro de experimentos registrados en MLflow, a su vez se guardaron los artefactos necesarios para reproducir los experiemntos el codificador y el vectorizador. \n",
    "\n",
    "Después la task selectModels utiliza MLflowClient para identificar y seleccionar los dos mejores modelos según su precisión. El modelo con mejor desempeño es etiquetado como Champion, y el segundo como Challenger. Ambos se registran en el Model Registry de MLflow con sus respectivos alias, facilitando la transición del modelo a producción.\n",
    "\n",
    "El flujo principal mainFlow orquesta todas estas tasks, integrando procesamiento, entrenamiento y selección de modelos de manera eficiente. \n",
    "\n",
    "  ‎\n",
    "\n",
    "\n",
    "<font color=\"lightsteelblue\">5. **Desarrollo de la API**  </font>\n",
    "\n",
    "**Archivo: `app/model/main.py`**\n",
    "\n",
    "   - Creé una API con FastAPI que permite realizar predicciones utilizando el modelo registrado como *Champion*.   \n",
    "   - Validé las entradas del usuario y devolví resultados en un formato que permitía la conexión de la UI.  \n",
    "   El backend, desarrollado con FastAPI, recibe solicitudes del frontend, procesa los datos de entrada con el modelo entrenado y devuelve las predicciones. Integra Pydantic para validar las entradas, carga dinámicamente el modelo Champion desde MLflow, transforma las predicciones numéricas a etiquetas legibles y expone el endpoint /predict para comunicar resultados en formato JSON.\n",
    "\n",
    "   ‎\n",
    "\n",
    "\n",
    "\n",
    "<font color=\"lavender\">6. **Desarrollo del frontend**</font>\n",
    "\n",
    "**Archivo: `app/UI/main.py`**\n",
    "\n",
    " El frontend está desarrollado con Streamlit, proporcionando una interfaz intuitiva y rápida para que los usuarios interactúen con la aplicación. A través de un formulario, los usuarios pueden ingresar texto describiendo sus quejas o comentarios y, al presionar un botón, activar el proceso de predicción. La interfaz, integrada con el backend de FastAPI, muestra en tiempo real la categoría de manera clara y accesible. \n",
    "\n",
    " ‎\n",
    "\n",
    "\n",
    "<font color=\"darkseagreen\">7. **Contenerización del proyecto**  </font>\n",
    "\n",
    "**Archivo: `app/docker-compose.yaml`** \n",
    "\n",
    "   - Usé el archivo `docker-compose.yaml` para orquestar los contenedores del frontend y backend\n",
    "   - Conecta el frontend y el backend para que trabajen juntos sin problemas. El frontend está en el puerto 8501, donde los usuarios ingresan datos, y el backend en el puerto 8000, parece procesar esa información y devolver los resultados. \n",
    "\n",
    "   ‎\n",
    "\n",
    "\n",
    "<font color=\"lightcoral\">8. **Conclusiones**  </font>\n",
    "\n",
    "   En este proyecto implementé un flujo completo de aprendizaje automático, desde la limpieza y análisis de datos hasta el desarrollo de una aplicación funcional con frontend y backend. Gracias a herramientas como MLflow y Prefect, pude lograr un proceso organizado y automatizado que facilita la reproducibilidad y escalabilidad de los modelos. Además, con los contenedores de Docker hubo una integración fluida entre los componentes de frontend y backend, ofreciendo una solución eficiente para la clasificación de quejas.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
