{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos librerías\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrimos el json, lo cargamos y normalizamos\n",
    "file_path = \"../data/raw_data/tickets_classification_eng.json\"\n",
    "with open(file_path, \"r\") as file:  \n",
    "    datos = json.load(file)\n",
    "\n",
    "df = pd.json_normalize(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos sólo las variables de interés\n",
    "df = df[['_source.complaint_what_happened', '_source.product', '_source.sub_product']]\n",
    "\n",
    "#Renombramos para facilitar el manejo de las columnas\n",
    "df = df.rename(columns={'_source.complaint_what_happened':'complaint_what_happened',\n",
    "                '_source.product':'category',\n",
    "                '_source.sub_product':'sub_product'\n",
    "                })\n",
    "\n",
    "#Creamos la nueva columna de clasificación\n",
    "df['ticket_classification'] = df['category'] + \" + \" + df['sub_product']\n",
    "\n",
    "#Dropeamos columnas redundantes\n",
    "df = df.drop(['category', 'sub_product'], axis=1)\n",
    "\n",
    "#Llenamos los registros vacíos con nulos de pandas\n",
    "df['complaint_what_happened'] = df['complaint_what_happened'].replace(\"\", pd.NA)\n",
    "\n",
    "#Dropeamos los nulos\n",
    "df = df.dropna()\n",
    "\n",
    "#Reseteamos el index\n",
    "df = df.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_what_happened</th>\n",
       "      <th>ticket_classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good morning my name is XXXX XXXX and I apprec...</td>\n",
       "      <td>Debt collection + Credit card debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I upgraded my XXXX XXXX card in XX/XX/2018 and...</td>\n",
       "      <td>Credit card or prepaid card + General-purpose ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chase Card was reported on XX/XX/2019. However...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>On XX/XX/2018, while trying to book a XXXX  XX...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>my grand son give me check for {$1600.00} i de...</td>\n",
       "      <td>Checking or savings account + Checking account</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              complaint_what_happened  \\\n",
       "1   Good morning my name is XXXX XXXX and I apprec...   \n",
       "2   I upgraded my XXXX XXXX card in XX/XX/2018 and...   \n",
       "10  Chase Card was reported on XX/XX/2019. However...   \n",
       "11  On XX/XX/2018, while trying to book a XXXX  XX...   \n",
       "14  my grand son give me check for {$1600.00} i de...   \n",
       "\n",
       "                                ticket_classification  \n",
       "1                  Debt collection + Credit card debt  \n",
       "2   Credit card or prepaid card + General-purpose ...  \n",
       "10  Credit reporting, credit repair services, or o...  \n",
       "11  Credit reporting, credit repair services, or o...  \n",
       "14     Checking or savings account + Checking account  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checamos que todo haya salido bien\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos en el directorio de data transformada\n",
    "df.to_csv('../data/transformed_data/preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a eliminar las palabras cuya frecuencia sobrepase el treshold, ya que existen palabras que se repiten mucho y no son precisamente stopwords, en especial chase, jp, y otros términos del banco que son casi constantes, por lo que no representan nada para el modelo y es mejor eliminarlas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_frequent_words(corpus, threshold=0.75):\n",
    "    # Creamos un CountVectorizer para obtener el conteo \n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Obtener la frecuencia de cada palabra\n",
    "    word_counts = X.sum(axis=0).A1  #Convertimos en un array unidimensional\n",
    "    word_list = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculamos el treshold en base al porcentaje y el tamaño del df\n",
    "    threshold_count = len(corpus) * threshold\n",
    "    \n",
    "    # Obtenemos las palabras que superen el treshold\n",
    "    frequent_words = {word_list[i] for i, count in enumerate(word_counts) if count > threshold_count}\n",
    "    \n",
    "    \n",
    "    filtered_corpus = []\n",
    "    for doc in corpus:\n",
    "        # Eliminamos esas palabras del corpus\n",
    "        filtered_doc = ' '.join([word for word in doc.split() if word not in frequent_words])\n",
    "        filtered_corpus.append(filtered_doc)\n",
    "    \n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquí vamos a hacer una pequeña limpieza, estirando contracciones a su forma base, convertir a minúsculas, eliminar todos los caracteres no alfanuméricos, las censuras y por último los stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpiamos otras cosillas con regex descontraemos y quitamos stopwords\n",
    "def clean_complaint(complaint):\n",
    "\n",
    "    #Convertimos a minúsculas\n",
    "    complaint = complaint.lower()\n",
    "\n",
    "    #Descontraemos\n",
    "    complaint = contractions.fix(complaint)\n",
    "\n",
    "    #Quitamos donde haya dos o más x\n",
    "    complaint = re.sub(r'xx+', '', complaint)\n",
    "    \n",
    "    #Eliminar números\n",
    "    #complaint = re.sub(r'\\d', '', complaint)\n",
    "\n",
    "    #Dejamos sólo alfanuméricos\n",
    "    complaint = re.sub(r'\\W', ' ', complaint)\n",
    "\n",
    "    # Tokenizamos y quitamos stopwords\n",
    "    complaint_tokens = word_tokenize(complaint)\n",
    "    complaint = ' '.join([word for word in complaint_tokens if word not in stop_words])\n",
    "\n",
    "    return complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento por batches\n",
    "batch_size = 1000\n",
    "cleaned_corpus = []\n",
    "\n",
    "for start in range(0, len(df), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch_complaints = df['complaint_what_happened'][start:end]\n",
    "    \n",
    "    # Limpiar cada complaint en cada batch\n",
    "    cleaned_batch = batch_complaints.apply(clean_complaint)\n",
    "    \n",
    "    # Appendear a la lista\n",
    "    cleaned_corpus.extend(cleaned_batch)\n",
    "\n",
    "cleaned_corpus = delete_frequent_words(cleaned_corpus, threshold=0.80)\n",
    "\n",
    "# Actualizar el df\n",
    "df['complaint_what_happened'] = cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_what_happened</th>\n",
       "      <th>ticket_classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good morning name appreciate could help put st...</td>\n",
       "      <td>Debt collection + Credit card debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>upgraded 2018 told agent upgrade anniversary d...</td>\n",
       "      <td>Credit card or prepaid card + General-purpose ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>reported 2019 however fraudulent application s...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018 trying book ticket came across offer 300 ...</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>grand son give check 1600 deposit fund clear c...</td>\n",
       "      <td>Checking or savings account + Checking account</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              complaint_what_happened  \\\n",
       "1   good morning name appreciate could help put st...   \n",
       "2   upgraded 2018 told agent upgrade anniversary d...   \n",
       "10  reported 2019 however fraudulent application s...   \n",
       "11  2018 trying book ticket came across offer 300 ...   \n",
       "14  grand son give check 1600 deposit fund clear c...   \n",
       "\n",
       "                                ticket_classification  \n",
       "1                  Debt collection + Credit card debt  \n",
       "2   Credit card or prepaid card + General-purpose ...  \n",
       "10  Credit reporting, credit repair services, or o...  \n",
       "11  Credit reporting, credit repair services, or o...  \n",
       "14     Checking or savings account + Checking account  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checamos que todo esté en orden\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos el treshold de rows mínimos por categoría, véase el EDA\n",
    "counts = df['ticket_classification'].value_counts()\n",
    "todelete = counts[counts < 10]\n",
    "df = df[~df['ticket_classification'].isin(todelete)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/clean_data/cleaned_corpus.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
